{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Eggs: reduced-form pass-through diagnostics\n",
    "\n",
    "This notebook is a step-by-step diagnostic for a rockets-and-feathers phenomenon in eggs.\n",
    "It is intentionally simple: one regression at a time, with explicit interpretation after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from linearmodels.iv import IV2SLS\n",
    "from scipy.stats import chi2\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We begin by downloading the data. All data are collected from public sources; see the documentation for details on data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Load data + prep\n",
    "# -----------------------\n",
    "df = pd.read_csv(\"/home/akimovh/rockets_feathers/data/reduced_form.csv\")\n",
    "retail_df = pd.read_csv(\"/home/akimovh/rockets_feathers/data/retail_price.csv\")\n",
    "\n",
    "df = pd.merge(df, retail_df, on=\"date\", how=\"inner\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Feed-cost proxy\n",
    "df[\"cost_feed_us\"] = 0.75 * df[\"cost_corn_us\"] + 0.25 * df[\"cost_sb_us\"] # Created in first iterations, this variable has low predictive power, so it's not used futher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Pass-through and adjustment dynamics in the U.S. egg market (reduced-form diagnostics)\n",
    "\n",
    "This notebook builds a **minimal, transparent set of reduced-form checks** before moving to an IV/ECM design. (CHANGE)\n",
    "\n",
    "**Data (monthly):**\n",
    "- **Wholesale / upstream price (`wh`)**: USDA weekly egg report aggregated to month (cents/dozen)\n",
    "- **Retail price (`rt`)**: BLS retail series (cents/dozen)\n",
    "- **Supply shock proxy (`death`)**: hen losses (rendered deaths), monthly flow\n",
    "\n",
    "**Roadmap**\n",
    "1. **Regression 1 (relevance of shifters):** Do plausibly exogenous supply shocks predict **wholesale price changes**?\n",
    "2. **Regression 2 (pass-through + timing diagnostics):** Does retail respond to wholesale, and how fast? (Include a lead placebo.)\n",
    "3. **Regression 2b (asymmetry in speed):** Are responses to wholesale increases vs decreases different by horizon?\n",
    "4. **Regression 3 (next step):** IV pass-through instrumenting short-run wholesale movements with avian-flu mortality windows (and their lags).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1) Wholesale price prediction / instrument relevance\n",
    "\n",
    "**Question.** Do plausibly exogenous supply shocks move the upstream egg price in a strong, predictable way?\n",
    "\n",
    "**Why this step?** If mortality shocks do *not* predict upstream price movements, they are not useful for IV later.\n",
    "\n",
    "**Specification choices**\n",
    "- Work in **monthly changes** (first differences): we want short-run movements and to avoid spurious trend relationships (feed and eggs costs might be trending parallel).\n",
    "- Control for **month-of-year fixed effects** to absorb seasonality (Easter/holiday baking, etc.).\n",
    "- Use **HAC** standard errors (monthly data + distributed lags ⇒ serial correlation likely).\n",
    "\n",
    "**Mortality windows (biological timing)**\n",
    "The production chain has adjustment delays: flock losses today can tighten supply immediately, but replacement takes months.\n",
    "We summarize mortality into three windows (in millions of hens):\n",
    "- `D_0_2`: contemporaneous + 1–2 month lag (near-term tightening)\n",
    "- `D_3_6`: 3–6 month lag (medium-term)\n",
    "- `D_7_12`: 7–12 month lag (longer-term recovery window)\n",
    "\n",
    "(Feed costs are a natural shifter, but in this dataset they have weaker predictive power for upstream changes; we keep them as an optional robustness control.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Build analysis df\n",
    "# -----------------------\n",
    "assym_df = df[[\"date\", \"cost_feed_us\", \"price_large\", \"loss_dth_render\"]].copy()\n",
    "assym_df.columns = [\"date\", \"cost\", \"price\", \"death\"]\n",
    "assym_df = assym_df.sort_values(\"date\")\n",
    "\n",
    "\n",
    "# Outcomes / shifters\n",
    "assym_df[\"d_price\"]  = assym_df[\"price\"].diff()           # cents/dozen\n",
    "assym_df[\"death_m\"]  = assym_df[\"death\"] / 1_000_000.0    # million hens (flow/shock)\n",
    "\n",
    "# Creating death variables\n",
    "\n",
    "for l in range(0, 13):\n",
    "    assym_df[f\"death_m_lag{l}\"] = assym_df[\"death_m\"].shift(l)\n",
    "\n",
    "assym_df[\"D_0_2\"]  = assym_df[[f\"death_m_lag{l}\" for l in range(0, 3)]].sum(axis=1)\n",
    "assym_df[\"D_3_6\"]  = assym_df[[f\"death_m_lag{l}\" for l in range(3, 7)]].sum(axis=1)\n",
    "assym_df[\"D_7_12\"] = assym_df[[f\"death_m_lag{l}\" for l in range(7, 13)]].sum(axis=1)\n",
    "\n",
    "# -----------------------\n",
    "# Seasonality controls (month-of-year FE)\n",
    "# -----------------------\n",
    "assym_df[\"month\"] = assym_df[\"date\"].dt.month\n",
    "month_dummies = pd.get_dummies(assym_df[\"month\"], prefix=\"m\", drop_first=True).astype(int)\n",
    "assym_df = pd.concat([assym_df, month_dummies], axis=1)\n",
    "\n",
    "# Drop NA created by differencing / lags\n",
    "assym_df = assym_df.dropna().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Regression 1: Δ wholesale on mortality windows (and month FE)\n",
    "# -----------------------\n",
    "y = assym_df[\"d_price\"]\n",
    "\n",
    "# Baseline: mortality windows only (plus month FE)\n",
    "X_cols = [\"D_0_2\", \"D_3_6\", \"D_7_12\"] + list(month_dummies.columns)\n",
    "\n",
    "# Optional robustness: add feed-cost changes (often weak here)\n",
    "# X_cols = ([\"D_0_2\", \"D_3_6\", \"D_7_12\", \"d_cost10\"]\n",
    "#          + [f\"d_cost10_lag{l}\" for l in range(1, K_feed + 1)]\n",
    "#          + list(month_dummies.columns))\n",
    "\n",
    "X = sm.add_constant(assym_df[X_cols])\n",
    "\n",
    "m1 = sm.OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 3})\n",
    "print(m1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Joint tests (block relevance)\n",
    "# -----------------------\n",
    "def joint_f_test(m1, X, cols):\n",
    "    R = np.zeros((len(cols), X.shape[1]))\n",
    "    for i, c in enumerate(cols):\n",
    "        R[i, X.columns.get_loc(c)] = 1.0\n",
    "    return m1.f_test(R)\n",
    "\n",
    "# feed_terms  = [\"d_cost10\"] + [f\"d_cost10_lag{l}\" for l in range(1, K_feed + 1)]\n",
    "death_terms = [\"D_0_2\", \"D_3_6\", \"D_7_12\"]\n",
    "# all_terms   = death_terms\n",
    "\n",
    "print(\"\\n--- Joint tests (H0: block = 0) ---\")\n",
    "# print(\"Feed block:\",  joint_f_test(model, X, feed_terms))\n",
    "print(\"Death block:\", joint_f_test(m1, X, death_terms))\n",
    "# print(\"All shifters:\", joint_f_test(model, X, all_terms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**Interpretation (Regression 1).**\n",
    "- The mortality windows have economically and statistically meaningful predictive power for upstream price changes (joint test).\n",
    "- Feed-cost changes are comparatively weak in this reduced-form (optional robustness rather than baseline (We exluded feed costs from final view of Regression 1)).\n",
    "\n",
    "**Implication for later.** Mortality windows look promising as *excluded shifters* for IV pass-through.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2) Retail response to wholesale changes (pass-through + timing)\n",
    "\n",
    "**Question.** Does retail price change when wholesale price changes, and how quickly?\n",
    "\n",
    "**Why differences?** Retail and wholesale levels are trending and highly persistent; the short-run adjustment dynamics are most transparently studied in **changes**.\n",
    "\n",
    "**Distributed-lag model (symmetric).**\n",
    "We regress monthly retail changes on contemporaneous and lagged wholesale changes:\n",
    "$$\n",
    "\\Delta p^R_t = \\alpha + \\sum_{k=0}^{K} \\beta_k \\Delta p^W_{t-k} + \\text{month FE} + u_t.\n",
    "$$\n",
    "The cumulative pass-through by horizon is $\\sum_{k=0}^h \\beta_k$.\n",
    "\n",
    "**Timing / endogeneity diagnostic (lead placebo).**\n",
    "We also include a small number of *leads* of wholesale changes (e.g., $\\Delta p^W_{t+1}, \\Delta p^W_{t+2}$).\n",
    "If leads predict $\\Delta p^R_t$, that suggests either:\n",
    "- dating/aggregation mismatch (monthly averaging), and/or\n",
    "- omitted persistent shocks common to both series.\n",
    "\n",
    "This diagnostic motivates IV in Regression 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Build analysis df\n",
    "# -----------------------\n",
    "assym_df = df[[\"date\", \"price_large\", 'retail_price']].copy()\n",
    "assym_df.columns = [\"date\", \"wh\", \"rt\"]\n",
    "assym_df = assym_df.sort_values(\"date\")\n",
    "\n",
    "assym_df[\"d_rt\"] = assym_df[\"rt\"].diff()     # Δ retail (cents/doz)\n",
    "assym_df[\"d_wh\"] = assym_df[\"wh\"].diff()     # Δ wholesale (cents/doz)\n",
    "\n",
    "K = 9     # number of wholesale lags\n",
    "L = 4     # number of wholesale leads (placebo test)\n",
    "\n",
    "# Leads and lags of Δ wholesale (note: negative lags are leads because shift(-1)=t+1)\n",
    "for l in range(-L, K + 1):\n",
    "    if l == 0:\n",
    "        continue\n",
    "    assym_df[f\"d_wh_lag{l}\"] = assym_df[\"d_wh\"].shift(l)\n",
    "\n",
    "# Month FE\n",
    "assym_df[\"month\"] = assym_df[\"date\"].dt.month\n",
    "month_dummies = pd.get_dummies(assym_df[\"month\"], prefix=\"m\", drop_first=True).astype(int)\n",
    "assym_df = pd.concat([assym_df, month_dummies], axis=1)\n",
    "\n",
    "# Drop NA created by diffs/lags/leads\n",
    "assym_df = assym_df.dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Visual check\n",
    "We start with a simple plot of retail vs wholesale levels to confirm co-movement and identify obvious breaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = assym_df.date\n",
    "\n",
    "# UC Berkeley colors\n",
    "berkeley_blue = \"#002676\"\n",
    "california_gold = \"#FDB515\"\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=assym_df.rt,\n",
    "    mode=\"lines\",\n",
    "    name=\"Retail price\",\n",
    "    line=dict(color=berkeley_blue, width=3.5)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=assym_df.wh,\n",
    "    mode=\"lines\",\n",
    "    name=\"Wholesale price\",\n",
    "    line=dict(color=california_gold, width=3.5)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Retail and wholesale price of eggs\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Price(cents/dozen)\",\n",
    "\n",
    "    # ECONOMETRICA / QJE STYLE\n",
    "    font=dict(\n",
    "        family=\"Times New Roman\",\n",
    "        size=14,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "    template=\"simple_white\",\n",
    "\n",
    "    # Axes styling\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgray\",\n",
    "        zeroline=False,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        ticks=\"outside\",\n",
    "        tickwidth=1,\n",
    "        tickcolor=\"black\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgray\",\n",
    "        zeroline=False,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        mirror=True,\n",
    "        ticks=\"outside\",\n",
    "        tickwidth=1,\n",
    "        tickcolor=\"black\"\n",
    "    ),\n",
    "\n",
    "    # Legend OUTSIDE (right)\n",
    "    legend=dict(\n",
    "        x=1.02,\n",
    "        y=1,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "        font=dict(size=15),\n",
    "        bgcolor=\"rgba(255,255,255,0)\",\n",
    "        bordercolor=\"rgba(0,0,0,0)\"\n",
    "    ),\n",
    "\n",
    "    margin=dict(r=150),\n",
    "    width=850,\n",
    "    height=520\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The series strongly co-move, especially during 2022–2025. Retail is smoother (consistent with menu costs/contracts/averaging).\n",
    "This motivates estimating a distributed-lag pass-through model in changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = assym_df[\"d_rt\"]\n",
    "\n",
    "X_cols = (\n",
    "    [\"d_wh\"] +\n",
    "    [f\"d_wh_lag{l}\" for l in range(-L, K + 1) if l != 0] +\n",
    "    list(month_dummies.columns)\n",
    ")\n",
    "\n",
    "X = sm.add_constant(assym_df[X_cols])\n",
    "\n",
    "m2 = sm.OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 12})\n",
    "print(m2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_cols = [\"d_wh\"] + [f\"d_wh_lag{l}\" for l in range(1, K + 1)]\n",
    "lead_cols = [f\"d_wh_lag{l}\" for l in range(-L, 0)]\n",
    "\n",
    "def joint_f_test(model, X, cols):\n",
    "    R = np.zeros((len(cols), X.shape[1]))\n",
    "    for i, c in enumerate(cols):\n",
    "        R[i, X.columns.get_loc(c)] = 1.0\n",
    "    return model.f_test(R)\n",
    "\n",
    "print(\"\\nWholesale lag block joint test (H0: all lags 0..K = 0):\")\n",
    "print(joint_f_test(m2, X, beta_cols))\n",
    "\n",
    "print(\"\\nWholesale lead placebo joint test (H0: all leads = 0):\")\n",
    "print(joint_f_test(m2, X, lead_cols))\n",
    "\n",
    "cum_pass = float(m2.params[beta_cols].sum())\n",
    "print(\"\\nCumulative pass-through (0..K):\", cum_pass)\n",
    "\n",
    "cum_path = m2.params[beta_cols].cumsum()\n",
    "print(\"\\nCumulative by lag:\")\n",
    "print(cum_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "**Interpretation (Regression 2).**\n",
    "- The wholesale distributed-lag block is strongly significant and cumulative pass-through is economically large.\n",
    "- The lead placebo can be statistically significant even when small in magnitude; this is a warning sign for clean causal interpretation of OLS dynamics (Need instruments for 3 lags)\n",
    "- We proceed with this OLS as a **diagnostic** and use IV later for causal claims.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Split wholesale changes into positive and negative (asymmetric dynamics)\n",
    "\n",
    "To test “rockets & feathers”-style asymmetry, we split wholesale changes:\n",
    "- `d_wh_pos = max(d_wh, 0)`\n",
    "- `d_wh_neg = min(d_wh, 0)` (non-positive)\n",
    "\n",
    "Then estimate a symmetric distributed-lag model with separate coefficients for positive vs negative wholesale changes and compute **cumulative differences by horizon**:\n",
    "$$\n",
    "\\sum_{k=0}^h \\beta^{+}_k - \\sum_{k=0}^h \\beta^{-}_k.\n",
    "$$\n",
    "\n",
    "This tells us whether retail adjusts faster to cost increases than to cost decreases at short horizons (speed asymmetry).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Build analysis df\n",
    "# -----------------------\n",
    "\n",
    "assym_df = df[[\"date\", \"price_large\", \"retail_price\"]].copy()\n",
    "assym_df.columns = [\"date\", \"wh\", \"rt\"]\n",
    "assym_df = assym_df.sort_values(\"date\")\n",
    "\n",
    "assym_df[\"d_rt\"] = assym_df[\"rt\"].diff()\n",
    "assym_df[\"d_wh\"] = assym_df[\"wh\"].diff()\n",
    "\n",
    "# Split wholesale changes\n",
    "assym_df[\"d_wh_pos\"] = assym_df[\"d_wh\"].clip(lower=0)\n",
    "assym_df[\"d_wh_neg\"] = assym_df[\"d_wh\"].clip(upper=0)   # ≤ 0\n",
    "\n",
    "K = 9  # horizon for asymmetric dynamics (kept short given sample size)\n",
    "\n",
    "# Lags for + and -\n",
    "for l in range(1, K + 1):\n",
    "    assym_df[f\"d_wh_pos_lag{l}\"] = assym_df[\"d_wh_pos\"].shift(l)\n",
    "    assym_df[f\"d_wh_neg_lag{l}\"] = assym_df[\"d_wh_neg\"].shift(l)\n",
    "\n",
    "# Month FE\n",
    "assym_df[\"month\"] = assym_df[\"date\"].dt.month\n",
    "month_dummies = pd.get_dummies(assym_df[\"month\"], prefix=\"m\", drop_first=True).astype(int)\n",
    "assym_df = pd.concat([assym_df, month_dummies], axis=1)\n",
    "\n",
    "assym_df = assym_df.dropna().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Regression 2b: asymmetric pass-through (Δ wholesale + vs -)\n",
    "# -----------------------\n",
    "\n",
    "y = assym_df[\"d_rt\"]\n",
    "\n",
    "pos_cols = [\"d_wh_pos\"] + [f\"d_wh_pos_lag{l}\" for l in range(1, K + 1)]\n",
    "neg_cols = [\"d_wh_neg\"] + [f\"d_wh_neg_lag{l}\" for l in range(1, K + 1)]\n",
    "\n",
    "X_cols = pos_cols + neg_cols + list(month_dummies.columns)\n",
    "X = sm.add_constant(assym_df[X_cols])\n",
    "\n",
    "m2b = sm.OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": K})\n",
    "print(m2b.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cols = [\"d_wh_pos\"] + [f\"d_wh_pos_lag{l}\" for l in range(1, K+1)]\n",
    "neg_cols = [\"d_wh_neg\"] + [f\"d_wh_neg_lag{l}\" for l in range(1, K+1)]\n",
    "\n",
    "# cumulative differences by horizon\n",
    "for h in range(0, K+1):\n",
    "    pos_h = pos_cols[:h+1]\n",
    "    neg_h = neg_cols[:h+1]\n",
    "\n",
    "    w = np.zeros(X.shape[1])\n",
    "    for c in pos_h:\n",
    "        w[X.columns.get_loc(c)] += 1.0\n",
    "    for c in neg_h:\n",
    "        w[X.columns.get_loc(c)] -= 1.0\n",
    "\n",
    "    test = m2b.wald_test(w, scalar=True)\n",
    "    diff = float(m2b.params[pos_h].sum() - m2b.params[neg_h].sum())\n",
    "\n",
    "    print(f\"h={h:2d}  cum_diff={diff: .4f}   chi2={float(test.statistic): .3f}   p={float(test.pvalue): .3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "**Interpretation (Regression 2b).**\n",
    "The horizon-by-horizon Wald tests show evidence consistent with **asymmetry in adjustment speed** (at some horizons),\n",
    "even if the “full-horizon” cumulative difference is not always significant.\n",
    "\n",
    "**Next step.** Because wholesale changes may reflect common shocks or timing mismatch (lead placebo),\n",
    "we move to an IV design that isolates supply-driven wholesale variation using mortality windows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 3) Adding IV (Regression 3)\n",
    "\n",
    "**Goal.** Estimate pass-through using only the component of wholesale price changes driven by plausibly exogenous supply shocks.\n",
    "\n",
    "**Endogeneity concern.** Even if egg demand is “stable,” wholesale changes can still co-move with omitted shocks (and monthly averaging can create lead predictability).\n",
    "So we avoid interpreting OLS dynamics as causal.\n",
    "\n",
    "**IV design (baseline).**\n",
    "- Treat the *short-run* wholesale changes as endogenous: $\\Delta p^W_t, \\Delta p^W_{t-1}, \\Delta p^W_{t-2}, \\Delta p^W_{t-3}$.\n",
    "- Include longer lags (4–9) as controls.\n",
    "- Use mortality windows as excluded instruments **and include their lags (0–3)** to strengthen the first stage for lagged wholesale changes.\n",
    "\n",
    "We will evaluate:\n",
    "- first-stage strength for each endogenous regressor,\n",
    "- stability across alternative “endogenous block” choices (0 only vs 0–1 vs 0–2),\n",
    "- implied cumulative pass-through.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Build analysis df\n",
    "# -----------------------\n",
    "assym_df = (\n",
    "    df[[\"date\", \"price_large\", \"retail_price\", \"loss_dth_render\"]]\n",
    "    .rename(columns={\"price_large\": \"wh\", \"retail_price\": \"rt\", \"loss_dth_render\": \"death\"})\n",
    "    .sort_values(\"date\")\n",
    "    .assign(death_m=lambda x: x[\"death\"] / 1e6)\n",
    ")\n",
    "\n",
    "for lag in range(13):\n",
    "    assym_df[f\"death_m_lag{lag}\"] = assym_df[\"death_m\"].shift(lag)\n",
    "\n",
    "assym_df[\"D_0_2\"]  = assym_df[[f\"death_m_lag{l}\" for l in range(0, 3)]].sum(axis=1)\n",
    "assym_df[\"D_3_6\"]  = assym_df[[f\"death_m_lag{l}\" for l in range(3, 7)]].sum(axis=1)\n",
    "assym_df[\"D_7_12\"] = assym_df[[f\"death_m_lag{l}\" for l in range(7, 13)]].sum(axis=1)\n",
    "\n",
    "for window in [\"D_0_2\", \"D_3_6\", \"D_7_12\"]:\n",
    "    for lag in range(1, 4):\n",
    "        assym_df[f\"{window}_lag_{lag}\"] = assym_df[window].shift(lag)\n",
    "\n",
    "\n",
    "assym_df[\"d_rt\"]  = assym_df[\"rt\"].diff()           # cents/dozen\n",
    "assym_df[\"d_wh\"] = assym_df[\"wh\"].diff()     # $10/ton change\n",
    "\n",
    "# -----------------------\n",
    "# Lags: feed (short) + death (long windows)\n",
    "# -----------------------\n",
    "K = 9\n",
    "for l in range(1, K + 1):\n",
    "    assym_df[f\"d_wh_lag{l}\"] = assym_df[\"d_wh\"].shift(l)\n",
    "    \n",
    "# -----------------------\n",
    "# Month FE\n",
    "# -----------------------\n",
    "assym_df[\"month\"] = assym_df[\"date\"].dt.month\n",
    "month_dummies = pd.get_dummies(assym_df[\"month\"], prefix=\"m\", drop_first=True).astype(int)\n",
    "assym_df = pd.concat([assym_df, month_dummies], axis=1)\n",
    "\n",
    "# Drop NA created by diffs/lags\n",
    "assym_df = assym_df.dropna().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define variable blocks ---\n",
    "endog_cols = [\"d_wh\"] + [f\"d_wh_lag{l}\" for l in range(1, 4)]\n",
    "exog_cols  = [f\"d_wh_lag{l}\" for l in range(4, K + 1)] + list(month_dummies.columns)\n",
    "iv_cols    = (\n",
    "    [\"D_0_2\", \"D_3_6\", \"D_7_12\"]\n",
    "    + [f\"{w}_lag_{l}\" for w in [\"D_0_2\", \"D_3_6\", \"D_7_12\"] for l in range(1, 4)]\n",
    ")\n",
    "\n",
    "y      = assym_df[\"d_rt\"]\n",
    "endog  = assym_df[endog_cols]\n",
    "exog   = sm.add_constant(assym_df[exog_cols])\n",
    "instruments = assym_df[iv_cols]\n",
    "\n",
    "# --- Estimate ---\n",
    "m3 = IV2SLS(y, exog, endog, instruments).fit(\n",
    "    cov_type=\"kernel\", kernel=\"bartlett\", bandwidth=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "What we check next:\n",
    "\n",
    "1. **Main IV estimates** (are the key wholesale coefficients sensible?).\n",
    "2. **First-stage strength** for each endogenous regressor (weak IV would invalidate inference).\n",
    "3. **Cumulative pass-through** and **speed of adjustment** (how much happens immediately vs. later).\n",
    "4. **Specification diagnostics**: endogeneity tests (Wu–Hausman / Durbin) and overidentification tests (Sargan).\n",
    "5. **Sensitivity**: results across alternative endogenous blocks and HAC bandwidth choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_terms = [\"d_wh\"] + [f\"d_wh_lag{l}\" for l in range(1, 4)]\n",
    "\n",
    "# Point estimates + SEs just for key terms\n",
    "out = pd.DataFrame({\n",
    "    \"beta\": m3.params.reindex(key_terms),\n",
    "    \"se\":   m3.std_errors.reindex(key_terms),\n",
    "    \"t\":    (m3.params.reindex(key_terms) / m3.std_errors.reindex(key_terms)),\n",
    "    \"p\":    2 * (1 - stats.norm.cdf(np.abs(m3.params.reindex(key_terms) / m3.std_errors.reindex(key_terms))))\n",
    "})\n",
    "\n",
    "print(\"Main IV estimates\")\n",
    "print(out)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"First-stage strength\")\n",
    "print(m3.first_stage.diagnostics[['rsquared',  'partial.rsquared',  'shea.rsquared',      'f.stat',   'f.pval']])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Cumulative pass-through\")\n",
    "betas = m3.params.reindex(key_terms)\n",
    "cum0 = betas[\"d_wh\"]\n",
    "cum1 = betas[[\"d_wh\",\"d_wh_lag1\"]].sum()\n",
    "cum2 = betas[[\"d_wh\",\"d_wh_lag1\",\"d_wh_lag2\"]].sum()\n",
    "cum3 = betas[key_terms].sum()\n",
    "\n",
    "print(pd.Series({\"cum0\":cum0, \"cum1\":cum1, \"cum2\":cum2, \"cum3\":cum3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "**Interpretation (Regression 3, IV).**\n",
    "Instrumenting contemporaneous and short-lag wholesale changes with mortality windows yields **large, fast pass-through**:\n",
    "$\\beta_0 \\approx 0.26$, $\\beta_1 \\approx 0.44$, $\\beta_3 \\approx 0.12$ (lag 2 is small and not significant).\n",
    "Cumulatively, pass-through reaches **~0.70 by 1 month** and **~0.85 by 3 months**.\n",
    "The instruments are **very strong**, and **overidentifying restrictions are not rejected** (p $\\approx 0.63$–$0.67$).\n",
    "HAC bandwidth choices change SEs slightly but **do not change inference**.\n",
    "\n",
    "**Next step.**\n",
    "Use this IV setup to test **asymmetry** directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# 4. Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Build analysis df\n",
    "# -----------------------\n",
    "assym_df = (\n",
    "    df[[\"date\", \"price_large\", \"retail_price\", \"loss_dth_render\"]]\n",
    "    .rename(columns={\"price_large\": \"wh\", \"retail_price\": \"rt\", \"loss_dth_render\": \"death\"})\n",
    "    .sort_values(\"date\")\n",
    "    .assign(death_m=lambda x: x[\"death\"] / 1e6)\n",
    ")\n",
    "\n",
    "# ---- Mortality windows (same as before) ----\n",
    "for lag in range(13):\n",
    "    assym_df[f\"death_m_lag{lag}\"] = assym_df[\"death_m\"].shift(lag)\n",
    "\n",
    "assym_df[\"D_0_2\"]  = assym_df[[f\"death_m_lag{l}\" for l in range(0, 3)]].sum(axis=1)\n",
    "assym_df[\"D_3_6\"]  = assym_df[[f\"death_m_lag{l}\" for l in range(3, 7)]].sum(axis=1)\n",
    "assym_df[\"D_7_12\"] = assym_df[[f\"death_m_lag{l}\" for l in range(7, 13)]].sum(axis=1)\n",
    "\n",
    "for window in [\"D_0_2\", \"D_3_6\", \"D_7_12\"]:\n",
    "    for lag in range(1, 4):\n",
    "        assym_df[f\"{window}_lag_{lag}\"] = assym_df[window].shift(lag)\n",
    "\n",
    "# ---- Differences ----\n",
    "assym_df[\"d_rt\"] = assym_df[\"rt\"].diff()\n",
    "assym_df[\"d_wh\"] = assym_df[\"wh\"].diff()\n",
    "\n",
    "# -----------------------\n",
    "# Split wholesale changes into + and - (magnitudes)\n",
    "#   d_wh_pos >= 0 : increases\n",
    "#   d_wh_neg >= 0 : decreases (magnitude)\n",
    "# Interpretation: a $1 *decrease* corresponds to +1 in d_wh_neg\n",
    "# -----------------------\n",
    "\n",
    "assym_df[\"d_wh_pos\"] = assym_df[\"d_wh\"].clip(lower=0)   # >=0\n",
    "assym_df[\"d_wh_neg\"] = assym_df[\"d_wh\"].clip(upper=0)   # <=0  (SIGNED, not magnitude)\n",
    "\n",
    "K = 9\n",
    "for side in [\"pos\", \"neg\"]:\n",
    "    for l in range(0, K + 1):\n",
    "        if l == 0:\n",
    "            continue\n",
    "        assym_df[f\"d_wh_{side}_lag{l}\"] = assym_df[f\"d_wh_{side}\"].shift(l)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Month FE\n",
    "# -----------------------\n",
    "assym_df[\"month\"] = assym_df[\"date\"].dt.month\n",
    "month_dummies = pd.get_dummies(assym_df[\"month\"], prefix=\"m\", drop_first=True).astype(int)\n",
    "assym_df = pd.concat([assym_df, month_dummies], axis=1)\n",
    "\n",
    "# Drop NA created by diffs/lags\n",
    "assym_df = assym_df.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Variable blocks (REG 4: asymmetry)\n",
    "# Endog: contemporaneous + lags 1-3 for BOTH + and -\n",
    "# Exog : lags 4..K for BOTH + and - + month FE\n",
    "# IV   : mortality windows (same)\n",
    "# -----------------------\n",
    "endog_cols = (\n",
    "    [\"d_wh_pos\", \"d_wh_neg\"]\n",
    "    + [f\"d_wh_pos_lag{l}\" for l in range(1, 4)]\n",
    "    + [f\"d_wh_neg_lag{l}\" for l in range(1, 4)]\n",
    ")\n",
    "\n",
    "exog_cols = (\n",
    "    [f\"d_wh_pos_lag{l}\" for l in range(0, K + 1) if l not in range(0,4)]\n",
    "    + [f\"d_wh_neg_lag{l}\" for l in range(0, K + 1) if l not in range(0,4)]\n",
    "    + list(month_dummies.columns)\n",
    ")\n",
    "\n",
    "iv_cols = (\n",
    "    [\"D_0_2\", \"D_3_6\", \"D_7_12\"]\n",
    "    + [f\"{w}_lag_{l}\" for w in [\"D_0_2\", \"D_3_6\", \"D_7_12\"] for l in range(1, 4)]\n",
    ")\n",
    "\n",
    "y = assym_df[\"d_rt\"]\n",
    "endog = assym_df[endog_cols]\n",
    "exog = sm.add_constant(assym_df[exog_cols])\n",
    "instruments = assym_df[iv_cols]\n",
    "\n",
    "# --- Estimate ---\n",
    "m4 = IV2SLS(y, exog, endog, instruments).fit(\n",
    "    cov_type=\"kernel\", kernel=\"bartlett\", bandwidth=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1) Main IV estimates (pos/neg only, 0..3)\n",
    "# -----------------------\n",
    "key_terms = (\n",
    "    [\"d_wh_pos\", \"d_wh_neg\"]\n",
    "    + [f\"d_wh_pos_lag{l}\" for l in range(1, 4)]\n",
    "    + [f\"d_wh_neg_lag{l}\" for l in range(1, 4)]\n",
    ")\n",
    "\n",
    "key_terms_adj = (\n",
    "    [\"d_wh_pos\", \"d_wh_neg\"]\n",
    "    + [f\"d_wh_pos_lag{l}\" for l in range(1, 4)]\n",
    "    + [f\"d_wh_neg_lag{l}\" for l in range(1, 4)]\n",
    ")\n",
    "\n",
    "b = m4.params.reindex(key_terms_adj)\n",
    "se = m4.std_errors.reindex(key_terms_adj)\n",
    "t = b / se\n",
    "p = 2 * (1 - stats.norm.cdf(np.abs(t)))\n",
    "\n",
    "out = pd.DataFrame({\"beta\": b, \"se\": se, \"t\": t, \"p\": p})\n",
    "\n",
    "print(\"Main IV estimates (Reg 4)\")\n",
    "print(out)\n",
    "print()\n",
    "\n",
    "# -----------------------\n",
    "# 2) First-stage strength (same terms)\n",
    "# -----------------------\n",
    "print(\"First-stage strength\")\n",
    "print(m4.first_stage.diagnostics.loc[key_terms, ['partial.rsquared', 'f.stat', 'f.pval']])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = m4.cov\n",
    "if not isinstance(V, pd.DataFrame):\n",
    "    V = pd.DataFrame(V, index=m4.params.index, columns=m4.params.index)\n",
    "\n",
    "rows = []\n",
    "for h in [0, 1, 2, 3]:\n",
    "    pos_terms = [\"d_wh_pos\"] + [f\"d_wh_pos_lag{l}\" for l in range(1, h+1)]\n",
    "    neg_terms = [\"d_wh_neg\"] + [f\"d_wh_neg_lag{l}\" for l in range(1, h+1)]\n",
    "\n",
    "    # cumulative COEFFICIENTS (both should be comparable magnitudes in this signed spec)\n",
    "    cum_pos = float(m4.params.reindex(pos_terms).sum())\n",
    "    cum_neg = float(m4.params.reindex(neg_terms).sum())\n",
    "\n",
    "    # H0 no asymmetry: cum_pos - cum_neg = 0\n",
    "    terms = pos_terms + neg_terms\n",
    "    w = np.r_[np.ones(len(pos_terms)), -np.ones(len(neg_terms))]  # + for pos, - for neg\n",
    "    bb = m4.params.reindex(terms).values.astype(float)\n",
    "    VV = V.loc[terms, terms].values.astype(float)\n",
    "\n",
    "    diff = float(w @ bb)              # cum_pos - cum_neg\n",
    "    var  = float(w @ VV @ w)\n",
    "    chi2 = (diff**2) / var\n",
    "    pval = 1 - stats.chi2.cdf(chi2, df=1)\n",
    "\n",
    "    rows.append({\"h\": h, \"cum_pos\": cum_pos, \"cum_neg\": cum_neg,\n",
    "                 \"diff (pos - neg)\": diff, \"p_no_asym\": pval})\n",
    "\n",
    "cum_tbl = pd.DataFrame(rows).set_index(\"h\")\n",
    "print(\"Cumulative pass-through + no-asymmetry test (H0: cum_pos - cum_neg = 0)\")\n",
    "print(cum_tbl)\n",
    "\n",
    "# -----------------------\n",
    "# Plot aligned with your template\n",
    "# -----------------------\n",
    "berkeley_blue = \"#002676\"\n",
    "california_gold = \"#FDB515\"\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cum_tbl.index,\n",
    "    y=cum_tbl[\"cum_pos\"],\n",
    "    mode=\"lines+markers\",\n",
    "    name=\"Cumulative pass-through (wh increase)\",\n",
    "    line=dict(color=berkeley_blue, width=3.5),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cum_tbl.index,\n",
    "    y=cum_tbl[\"cum_neg\"],\n",
    "    mode=\"lines+markers\",\n",
    "    name=\"Cumulative pass-through (wh decrease)\",\n",
    "    line=dict(color=california_gold, width=3.5),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Cumulative pass-through by horizon (Regression 4)\",\n",
    "    xaxis_title=\"Horizon (months)\",\n",
    "    yaxis_title=\"Cumulative pass-through\",\n",
    "\n",
    "    font=dict(family=\"Times New Roman\", size=14, color=\"black\"),\n",
    "    template=\"simple_white\",\n",
    "\n",
    "    xaxis=dict(\n",
    "        showgrid=True, gridcolor=\"lightgray\", zeroline=False,\n",
    "        linecolor=\"black\", linewidth=1, mirror=True,\n",
    "        ticks=\"outside\", tickwidth=1, tickcolor=\"black\",\n",
    "        dtick=1\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True, gridcolor=\"lightgray\", zeroline=False,\n",
    "        linecolor=\"black\", linewidth=1, mirror=True,\n",
    "        ticks=\"outside\", tickwidth=1, tickcolor=\"black\"\n",
    "    ),\n",
    "\n",
    "    legend=dict(\n",
    "        x=1.02, y=1, xanchor=\"left\", yanchor=\"top\",\n",
    "        font=dict(size=15),\n",
    "        bgcolor=\"rgba(255,255,255,0)\",\n",
    "        bordercolor=\"rgba(0,0,0,0)\"\n",
    "    ),\n",
    "\n",
    "    margin=dict(r=150),\n",
    "    width=850,\n",
    "    height=520\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Takeaways and next steps\n",
    "\n",
    "Using monthly data, I do **not** find robust evidence of **asymmetric retail adjustment** to wholesale shocks (i.e., no clear “rockets and feathers” pattern) in the specifications explored above. Point estimates sometimes differ across positive vs. negative wholesale changes, but these differences are **not stable across horizons/specifications** and are difficult to distinguish from noise.\n",
    "\n",
    "This null result has two interpretations:\n",
    "1. **True symmetry:** retail egg prices may adjust roughly symmetrically to wholesale movements in this period.\n",
    "2. **Data and frequency limitations:** with a relatively short monthly sample and aggregation from weekly wholesale reports, the analysis may be underpowered to detect asymmetry that operates at higher frequency (e.g., within-week pricing, promotions, or category management).\n",
    "\n",
    "**Next step.** Replicate the same design using **weekly retail scanner data (Nielsen)** (and potentially regional variation) to increase statistical power and to better align the timing of retail and wholesale movements. If asymmetry exists primarily in adjustment speed rather than long-run pass-through, weekly data should make it substantially easier to detect.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
