{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports + constants\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from dateutil import parser as dateparser\n",
    "\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/.config/mysecrets/env\"))\n",
    "NASS_KEY = os.environ[\"NASS_KEY\"]\n",
    "NASS_BASE = \"https://quickstats.nass.usda.gov/api\"\n",
    "\n",
    "AMS_BASE = \"https://marsapi.ams.usda.gov/services/v1.2\"\n",
    "AMS_KEY = os.environ[\"AMS_KEY\"]  # or whatever env var you used\n",
    "\n",
    "# Physical constants — document units and sources\n",
    "LB_PER_BUSHEL_SOY = 60.0                # USDA standard weight\n",
    "LB_PER_METRIC_TON = 2_204.622_621_85    # exact SI definition\n",
    "BUSHELS_PER_METRIC_TON_SOY = LB_PER_METRIC_TON / LB_PER_BUSHEL_SOY  # ≈ 36.744\n",
    "\n",
    "LB_PER_BUSHEL_CORN = 56.0\n",
    "BUSHELS_PER_METRIC_TON_CORN = LB_PER_METRIC_TON / LB_PER_BUSHEL_CORN  # ≈ 39.369\n",
    "\n",
    "MONTH_MAP: dict[str, int] = {\n",
    "    \"JAN\": 1, \"FEB\": 2, \"MAR\": 3, \"APR\": 4,  \"MAY\": 5,  \"JUN\": 6,\n",
    "    \"JUL\": 7, \"AUG\": 8, \"SEP\": 9, \"OCT\": 10, \"NOV\": 11, \"DEC\": 12,\n",
    "}\n",
    "\n",
    "MONTH_MAP_2: dict[str, int] = {\n",
    "    \"FIRST OF JAN\": 1, \"FIRST OF FEB\": 2, \"FIRST OF MAR\": 3, \"FIRST OF APR\": 4,  \"FIRST OF MAY\": 5,  \"FIRST OF JUN\": 6,\n",
    "    \"FIRST OF JUL\": 7, \"FIRST OF AUG\": 8, \"FIRST OF SEP\": 9, \"FIRST OF OCT\": 10, \"FIRST OF NOV\": 11, \"FIRST OF DEC\": 12,\n",
    "\n",
    "}\n",
    "\n",
    "FOLDER = '/home/akimovh/rockets_feathers/eggs/' # enter the directory you are working in\n",
    "\n",
    "TXT_DATA_DIR = Path(FOLDER + \"data/2848_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "\n",
    "def column_filter_format(df:pd.DataFrame, value_name:str) -> pd.DataFrame:\n",
    "    required_cols = {\"year\", \"reference_period_desc\", \"Value\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Expected columns missing from API response: {missing}\")\n",
    "    df = df[list(required_cols)].rename(columns={\n",
    "        \"reference_period_desc\": \"month\",\n",
    "        \"Value\":                 value_name,})\n",
    "    return df\n",
    "\n",
    "def parse_value(df:pd.DataFrame, value_name:str) -> pd.DataFrame:\n",
    "    df[value_name] = (\n",
    "        df[value_name]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)   \n",
    "        .pipe(pd.to_numeric, errors=\"coerce\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def data_filter_format(df:pd.DataFrame, month_map:dict) -> pd.DataFrame:\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"month\"] = df[\"month\"].str.upper().map(month_map)\n",
    "    unmapped = df[\"month\"].isna()\n",
    "    if unmapped.any():\n",
    "        bad_rows = df.loc[unmapped]\n",
    "        raise ValueError(f\"Unmapped month values:\\n{bad_rows}\")\n",
    "    df[\"month\"] = df[\"month\"].astype(\"Int64\")\n",
    "    df[\"date\"] = pd.to_datetime(dict(year=df[\"year\"], month=df[\"month\"], day=1))\n",
    "    df = df.drop(['year', 'month'], axis = 1)\n",
    "    \n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# NASS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Feed prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Soy prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction\n",
    "params = {\n",
    "    \"key\":              NASS_KEY,\n",
    "    \"format\":           \"JSON\",\n",
    "    \"source_desc\":      \"SURVEY\",\n",
    "    \"sector_desc\":      \"CROPS\",\n",
    "    \"group_desc\":       \"FIELD CROPS\",\n",
    "    \"commodity_desc\":   \"SOYBEANS\",\n",
    "    \"statisticcat_desc\":\"PRICE RECEIVED\",\n",
    "    \"agg_level_desc\":   \"NATIONAL\",\n",
    "    \"freq_desc\":        \"MONTHLY\",\n",
    "    \"unit_desc\":        \"$ / BU\",\n",
    "    \"year__GE\":         1990,\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    f\"{NASS_BASE}/api_GET/\",\n",
    "    params=params,\n",
    "    timeout=120,\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "raw_data = response.json().get(\"data\", [])\n",
    "if not raw_data:\n",
    "    raise ValueError(\"NASS API returned empty dataset — check query parameters\")\n",
    "\n",
    "soy_prices = pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting\n",
    "\n",
    "soy_prices = column_filter_format(soy_prices, \"cost_sb_us\")\n",
    "soy_prices = parse_value(soy_prices, \"cost_sb_us\")\n",
    "soy_prices = data_filter_format(soy_prices, MONTH_MAP)\n",
    "soy_prices[\"cost_sb_us\"] = soy_prices[\"cost_sb_us\"] * BUSHELS_PER_METRIC_TON_SOY\n",
    "\n",
    "soy_prices.to_csv(FOLDER + \"data/usda/cost_sb_us.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Corn prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction\n",
    "params = {\n",
    "    \"key\": NASS_KEY,\n",
    "    \"format\": \"JSON\",\n",
    "    \"source_desc\": \"SURVEY\",\n",
    "    \"sector_desc\": \"CROPS\",\n",
    "    \"group_desc\": \"FIELD CROPS\",\n",
    "    \"commodity_desc\": \"CORN\",\n",
    "    \"statisticcat_desc\": \"PRICE RECEIVED\",\n",
    "    \"agg_level_desc\": \"NATIONAL\",\n",
    "    \"unit_desc\":        \"$ / BU\",\n",
    "    \"freq_desc\": \"MONTHLY\",\n",
    "    \"year__GE\": 1990,\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    f\"{NASS_BASE}/api_GET/\",\n",
    "    params=params,\n",
    "    timeout=120,\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "raw_data = response.json().get(\"data\", [])\n",
    "if not raw_data:\n",
    "    raise ValueError(\"NASS API returned empty dataset — check query parameters\")\n",
    "\n",
    "corn_prices = pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting\n",
    "\n",
    "corn_prices = column_filter_format(corn_prices, \"cost_corn_us\")\n",
    "corn_prices = parse_value(corn_prices, \"cost_corn_us\")\n",
    "corn_prices = data_filter_format(corn_prices, MONTH_MAP)\n",
    "corn_prices[\"cost_corn_us\"] = corn_prices[\"cost_corn_us\"] * BUSHELS_PER_METRIC_TON_CORN\n",
    "\n",
    "corn_prices.to_csv(FOLDER + \"data/usda/cost_corn_us.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Poultry data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Egg inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"key\": NASS_KEY,\n",
    "    \"format\": \"JSON\",\n",
    "    \"source_desc\": \"SURVEY\",\n",
    "    \"sector_desc\": \"ANIMALS & PRODUCTS\",\n",
    "    \"group_desc\": \"POULTRY\",\n",
    "    \"commodity_desc\": \"EGGS\",\n",
    "    \"short_desc\": \"EGGS, TABLE - PRODUCTION, MEASURED IN DOZEN\",\n",
    "    \"agg_level_desc\": \"NATIONAL\",\n",
    "    \"freq_desc\": \"MONTHLY\",\n",
    "    \"year__GE\": 2000,\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{NASS_BASE}/api_GET/\", params=params, timeout=120)\n",
    "r.raise_for_status()\n",
    "\n",
    "egg_prod_doz = pd.DataFrame(r.json().get(\"data\", []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting\n",
    "\n",
    "egg_prod_doz = column_filter_format(egg_prod_doz, \"egg_prod_doz\")\n",
    "egg_prod_doz = parse_value(egg_prod_doz, \"egg_prod_doz\")\n",
    "egg_prod_doz = data_filter_format(egg_prod_doz, MONTH_MAP)\n",
    "\n",
    "egg_prod_doz.to_csv(FOLDER + \"data/usda/egg_prod_doz.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Layers inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"key\": NASS_KEY,\n",
    "    \"format\": \"JSON\",\n",
    "    \"source_desc\": \"SURVEY\",\n",
    "    \"sector_desc\": \"ANIMALS & PRODUCTS\",\n",
    "    \"group_desc\": \"POULTRY\",\n",
    "    \"commodity_desc\": \"CHICKENS\",\n",
    "    \"statisticcat_desc\": \"INVENTORY\",\n",
    "    \"class_desc\": \"LAYERS, TABLE\",\n",
    "    \"agg_level_desc\": \"NATIONAL\",\n",
    "    \"year__GE\": 2000,\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{NASS_BASE}/api_GET/\", params=params, timeout=120)\n",
    "r.raise_for_status()\n",
    "\n",
    "layer_inv = pd.DataFrame(r.json().get(\"data\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting\n",
    "\n",
    "layer_inv = column_filter_format(layer_inv, \"layer_inv\")\n",
    "layer_inv = parse_value(layer_inv, \"layer_inv\")\n",
    "layer_inv = data_filter_format(layer_inv, MONTH_MAP_2)\n",
    "\n",
    "layer_inv.to_csv(FOLDER + \"data/usda/layer_inv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Layers loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"key\": NASS_KEY,\n",
    "    \"format\": \"JSON\",\n",
    "    \"source_desc\": \"SURVEY\",\n",
    "    \"sector_desc\": \"ANIMALS & PRODUCTS\",\n",
    "    \"group_desc\": \"POULTRY\",\n",
    "    \"commodity_desc\": \"CHICKENS\",\n",
    "    \"statisticcat_desc\": \"LOSS, DEATH & RENDERED\",\n",
    "    \"class_desc\": \"LAYERS\",\n",
    "    \"unit_desc\": \"HEAD\",\n",
    "    \"agg_level_desc\": \"NATIONAL\",\n",
    "    \"freq_desc\": \"MONTHLY\",\n",
    "    \"year__GE\": 2000,\n",
    "}\n",
    "\n",
    "r = requests.get(f\"{NASS_BASE}/api_GET/\", params=params, timeout=120)\n",
    "r.raise_for_status()\n",
    "\n",
    "loss_dth_render = pd.DataFrame(r.json().get(\"data\", []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting\n",
    "\n",
    "loss_dth_render = column_filter_format(loss_dth_render, \"loss_dth_render\")\n",
    "loss_dth_render = parse_value(loss_dth_render, \"loss_dth_render\")\n",
    "loss_dth_render = data_filter_format(loss_dth_render, MONTH_MAP)\n",
    "\n",
    "loss_dth_render.to_csv(FOLDER + \"data/usda/loss_dth_render.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# AMS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Egg prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Old data (in txt format, downloaded manually from MMN archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1890\n",
      "Files with errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Pasing txt files\n",
    "\n",
    "# Template to parse date\n",
    "HEADER_RE = re.compile(\n",
    "    r\"^Washington,\\s*DC\\s+(?P<date>(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun)\\.\\s+\"\n",
    "    r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4})\\s+\"\n",
    "    r\"USDA\\s+Market\\s+News\\s*$\"\n",
    ")\n",
    "\n",
    "def parse_one_file(fp: Path) -> pd.DataFrame:\n",
    "    text = fp.read_text(encoding=\"latin-1\", errors=\"replace\")\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # 1) date from header line (guaranteed present in this format)\n",
    "    date_str = next(HEADER_RE.match(line.strip()).group(\"date\")\n",
    "                    for line in lines if HEADER_RE.match(line.strip()))\n",
    "    date = pd.to_datetime(date_str, format=\"%a. %b %d, %Y\").normalize()\n",
    "\n",
    "    # 2) find table start: line after \"REGIONS ...\"\n",
    "    start = next(i for i, line in enumerate(lines) if line.strip().startswith(\"REGIONS\")) + 1\n",
    "\n",
    "    # 3) collect rows until footer marker\n",
    "    rows = []\n",
    "    for line in lines[start:]:\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if s.startswith((\"Computed\", \"Source:\", \"Prepared:\")):\n",
    "            break\n",
    "\n",
    "        parts = s.split()\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "\n",
    "        region = \" \".join(parts[:-3])\n",
    "        ex_large, large, medium = map(float, parts[-3:])\n",
    "\n",
    "        rows.append(\n",
    "            {\"date\": date, \"region\": region, \"ex_large\": ex_large, \"large\": large, \"medium\": medium}\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "files = sorted([p for p in TXT_DATA_DIR.iterdir() if p.is_file() and p.suffix.lower() == \".txt\"])\n",
    "\n",
    "dfs = []\n",
    "errors = []\n",
    "\n",
    "for fp in files:\n",
    "    try:\n",
    "        dfs.append(parse_one_file(fp))\n",
    "    except Exception as e:\n",
    "        errors.append((fp.name, str(e)))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True).sort_values([\"date\", \"region\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Files with errors:\", len(errors))\n",
    "if errors:\n",
    "    pd.DataFrame(errors, columns=[\"file\", \"error\"]).head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate (week_fri, region) rows: 0\n",
      "Canonical range: 2017-11-03 → 2025-01-24\n",
      "Unique canonical weeks: 378\n",
      "Expected Fridays: 378\n",
      "Missing weeks: 0\n",
      "Extra weeks: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>week_fri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>2017-11-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>2019-08-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>2020-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>2021-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>2021-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>2022-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   week_fri\n",
       "5    2017-11-09 2017-11-10\n",
       "390  2019-05-06 2019-05-03\n",
       "465  2019-08-19 2019-08-16\n",
       "695  2020-07-02 2020-07-03\n",
       "825  2020-12-31 2021-01-01\n",
       "1080 2021-12-23 2021-12-24\n",
       "1085 2021-12-30 2021-12-31\n",
       "1310 2022-11-10 2022-11-11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some of the reports published not on Fridays - moved them, so all dates exactly one week appart\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize()\n",
    "\n",
    "fri = pd.offsets.Week(weekday=4)  # Friday\n",
    "\n",
    "# Map each publication date to the nearest \"week-ending Friday\"\n",
    "# (Mon -> previous Fri; Thu -> next Fri; Fri -> same)\n",
    "def week_end_friday(d: pd.Timestamp) -> pd.Timestamp:\n",
    "    prev_fri = fri.rollback(d)     # Friday on/before d\n",
    "    next_fri = fri.rollforward(d)  # Friday on/after d\n",
    "    # pick whichever Friday is closer in time\n",
    "    return prev_fri if (d - prev_fri) <= (next_fri - d) else next_fri\n",
    "\n",
    "df[\"week_fri\"] = df[\"date\"].apply(week_end_friday)\n",
    "\n",
    "# ---- checks on canonical week index ----\n",
    "dup = df.duplicated([\"week_fri\", \"region\"]).sum()\n",
    "print(\"Duplicate (week_fri, region) rows:\", dup)\n",
    "\n",
    "dates = pd.DatetimeIndex(sorted(df[\"week_fri\"].unique()))\n",
    "expected = pd.date_range(dates.min(), dates.max(), freq=\"W-FRI\")\n",
    "\n",
    "missing = expected.difference(dates)\n",
    "extra = dates.difference(expected)\n",
    "\n",
    "print(\"Canonical range:\", dates.min().date(), \"→\", dates.max().date())\n",
    "print(\"Unique canonical weeks:\", len(dates))\n",
    "print(\"Expected Fridays:\", len(expected))\n",
    "print(\"Missing weeks:\", len(missing))\n",
    "print(\"Extra weeks:\", len(extra))\n",
    "\n",
    "# show the ones that moved\n",
    "moved = (\n",
    "    df.loc[df[\"date\"] != df[\"week_fri\"], [\"date\", \"week_fri\"]]\n",
    "      .drop_duplicates()\n",
    "      .sort_values(\"date\")\n",
    ")\n",
    "display(moved)\n",
    "\n",
    "df['date'] = df[\"week_fri\"]\n",
    "df = df.drop(\"week_fri\", axis = 1).copy()\n",
    "region_map = {\n",
    "    \"COMBINED REGIONAL\": \"National\",\n",
    "    \"MIDWEST\": \"Midwest\",\n",
    "    \"NORTHEAST\": \"Northeast\",\n",
    "    \"SOUTH CENTRAL\": \"South Central\",\n",
    "    \"SOUTHEAST\": \"Southeast\",\n",
    "}\n",
    "\n",
    "df[\"region\"] = df[\"region\"].map(region_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### New Data (now in pdf, but I get JSON through API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status: 200\n",
      "url: https://marsapi.ams.usda.gov/services/v1.2/reports/2848?q=report_begin_date%3D01%2F20%2F2025%3A01%2F14%2F2026&allSections=true\n"
     ]
    }
   ],
   "source": [
    "# Downloading\n",
    "\n",
    "SLUG = 2848\n",
    "start = \"01/20/2025\"\n",
    "end = pd.Timestamp.today().strftime(\"%m/%d/%Y\")\n",
    "\n",
    "r = requests.get(\n",
    "    f\"{AMS_BASE}/reports/{SLUG}\",\n",
    "    params={\"q\": f\"report_begin_date={start}:{end}\", \"allSections\": \"true\"},\n",
    "    auth=(AMS_KEY, \"\"),\n",
    "    timeout=120,\n",
    ")\n",
    "print(\"status:\", r.status_code)\n",
    "print(\"url:\", r.url)\n",
    "r.raise_for_status()\n",
    "\n",
    "rep_range = r.json()\n",
    "\n",
    "pdf_df = pd.DataFrame(rep_range[4]['results'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "\n",
    "pdf_df = pdf_df.query(\n",
    "    \"`class` in ['Extra Large', 'Large', 'Medium'] and delivery == 'Delivered Warehouse'\"\n",
    ")\n",
    "pdf_df = pdf_df[['report_begin_date', 'class', 'region', 'avg_price']]\n",
    "pdf_df = (\n",
    "    pdf_df\n",
    "    .pivot(\n",
    "        index=[\"report_begin_date\", \"region\"],\n",
    "        columns=\"class\",\n",
    "        values=\"avg_price\",\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "pdf_df.columns = df.columns\n",
    "pdf_df[\"date\"] = pd.to_datetime(pdf_df[\"date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "fri = pd.offsets.Week(weekday=4)  # Friday\n",
    "\n",
    "pdf_df[\"date\"] = pd.to_datetime(pdf_df[\"date\"], format=\"%m/%d/%Y\").dt.normalize()\n",
    "pdf_df[\"date\"] = pdf_df[\"date\"].map(fri.rollforward)  # Sunday->Friday, Friday->Friday\n",
    "pdf_df.ex_large = pdf_df.ex_large.astype(float)\n",
    "pdf_df.large = pdf_df.large.astype(float)\n",
    "pdf_df.medium = pdf_df.medium.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting old and saving \n",
    "df = pd.concat([df, pdf_df]).sort_values([\"date\", \"region\"])\n",
    "\n",
    "df = df.drop_duplicates().copy()\n",
    "\n",
    "df.to_csv(FOLDER + \"data/usda/warehouse_price.csv\", index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating weekly data to monthly\n",
    "df[\"week_start\"] = df[\"date\"] - pd.Timedelta(days=6)  # Sunday\n",
    "df[\"week_end\"]   = df[\"date\"] # Saturday\n",
    "\n",
    "def expand_week_to_months(row):\n",
    "    dates = pd.date_range(row.week_start, row.week_end, freq=\"D\")\n",
    "    return (\n",
    "        pd.DataFrame({\n",
    "            \"date\": dates,\n",
    "            \"month\": dates.to_period(\"M\").to_timestamp(),\n",
    "            \"price_ex_large\": row.ex_large,\n",
    "            \"price_large\": row.large,\n",
    "            \"price_medium\": row.medium,\n",
    "            \"region\": row.region,\n",
    "        })\n",
    "    )\n",
    "\n",
    "expanded_df = pd.concat(\n",
    "    df.apply(expand_week_to_months, axis=1).tolist(),\n",
    "    ignore_index=True,\n",
    ")\n",
    "monthly_df = (\n",
    "    expanded_df\n",
    "    .groupby([\"month\", \"region\"], as_index=False)\n",
    "    .mean()\n",
    ")[['month', 'region', 'price_ex_large', 'price_large', 'price_medium']]\n",
    "\n",
    "monthly_df.columns = ['date', 'region', 'price_ex_large', 'price_large', 'price_medium']\n",
    "\n",
    "monthly_df.to_csv(FOLDER + \"data/usda/warehouse_price_monthly.csv\", index = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Creating one reduced form df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-10-01 00:00:00')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(soy_prices.date.min(), corn_prices.date.min(), egg_prod_doz.date.min(), layer_inv.date.min(), loss_dth_render.date.min(), monthly_df.date.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "I am gonna do 2018-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "dfs = [\n",
    "    soy_prices,\n",
    "    corn_prices,\n",
    "    egg_prod_doz,\n",
    "    layer_inv,\n",
    "    loss_dth_render,\n",
    "    monthly_df.query(\"region == 'National'\"),\n",
    "]\n",
    "\n",
    "df = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=\"date\", how=\"inner\"),\n",
    "    dfs,\n",
    ")\n",
    "\n",
    "df = df.query(\"date >= '2018-01-01' and date < '2025-10-01'\")\n",
    "\n",
    "df.to_csv(FOLDER + \"data/usda/reduced_form.csv\", index = 0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
